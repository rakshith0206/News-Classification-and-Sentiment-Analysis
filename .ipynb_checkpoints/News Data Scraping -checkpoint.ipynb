{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install feedparser\n",
    "#!pip install newspaper3k\n",
    "\n",
    "import feedparser as fp\n",
    "import numpy as np\n",
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the limit for number of articles to download\n",
    "LIMIT = 1000000000\n",
    "articles_array = []\n",
    "\n",
    "data = {}\n",
    "data['newspapers'] = {}\n",
    "\n",
    "# Loads the JSON files with news sites\n",
    "with open('NewsPapers.json') as data_file:\n",
    "    companies = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "# Iterate through each news company\n",
    "for company, value in companies.items():\n",
    "    # If a RSS link is provided in the JSON file, this will be the first choice.\n",
    "    # Reason for this is that, RSS feeds often give more consistent and correct data. RSS (Rich Site Summary; originally RDF Site Summary; often called Really Simple Syndication) is a type of\n",
    "    # web feed which allows users to access updates to online content in a standardized, computer-readable format\n",
    "    # If you do not want to scrape from the RSS-feed, just leave the RSS attr empty in the JSON file.\n",
    "    if 'rss' in value:\n",
    "        d = fp.parse(value['rss'])\n",
    "        print(\"Downloading articles from \", company)\n",
    "        newsPaper = {\n",
    "            \"rss\": value['rss'],\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        for entry in d.entries:\n",
    "            # Check if publish date is provided, if no the article is skipped.\n",
    "            # This is done to keep consistency in the data and to keep the script from crashing.\n",
    "            if hasattr(entry, 'published'):\n",
    "                if count > LIMIT:\n",
    "                    break\n",
    "                article = {}\n",
    "                article['link'] = entry.link\n",
    "                date = entry.published_parsed\n",
    "                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()\n",
    "                try:\n",
    "                    content = Article(entry.link)\n",
    "                    content.download()\n",
    "                    content.parse()\n",
    "                except Exception as e:\n",
    "                    # If the download for some reason fails (ex. 404) the script will continue downloading\n",
    "                    # the next article.\n",
    "                    print(e)\n",
    "                    print(\"continuing...\")\n",
    "                    continue\n",
    "                article['title'] = content.title\n",
    "                article['text'] = content.text\n",
    "                article['authors'] = content.authors\n",
    "                article['top_image'] =  content.top_image\n",
    "                article['movies'] = content.movies\n",
    "                newsPaper['articles'].append(article)\n",
    "                articles_array.append(article)\n",
    "                print(count, \"articles downloaded from\", company, \", url: \", entry.link)\n",
    "                count = count + 1\n",
    "    else:\n",
    "        # This is the fallback method if a RSS-feed link is not provided.\n",
    "        # It uses the python newspaper library to extract articles\n",
    "        print(\"Building site for \", company)\n",
    "        paper = newspaper.build(value['link'], memoize_articles=False)\n",
    "        newsPaper = {\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        noneTypeCount = 0\n",
    "        for content in paper.articles:\n",
    "            if count > LIMIT:\n",
    "                break\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "                continue\n",
    "            # Again, for consistency, if there is no found publish date the article will be skipped.\n",
    "            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
    "\n",
    "            article = {}\n",
    "            article['title'] = content.title\n",
    "            article['authors'] = content.authors\n",
    "            article['text'] = content.text\n",
    "            article['top_image'] =  content.top_image\n",
    "            article['movies'] = content.movies\n",
    "            article['link'] = content.url\n",
    "            article['published'] = content.publish_date\n",
    "            newsPaper['articles'].append(article)\n",
    "            articles_array.append(article)\n",
    "            print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n",
    "            count = count + 1\n",
    "            #noneTypeCount = 0\n",
    "    count = 1\n",
    "    data['newspapers'][company] = newsPaper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally it saves the articles as a CSV-file.\n",
    "try:\n",
    "    f = csv.writer(open('Scraped_data_news_output.csv', 'w', encoding='utf-8'))\n",
    "    f.writerow(['Title', 'Authors','Text','Image','Videos','Link','Published_Date'])\n",
    "    #print(article)\n",
    "    for artist_name in articles_array:\n",
    "        title = artist_name['title']\n",
    "        authors=artist_name['authors']\n",
    "        text=artist_name['text']\n",
    "        image=artist_name['top_image']\n",
    "        video=artist_name['movies']\n",
    "        link=artist_name['link']\n",
    "        publish_date=artist_name['published']\n",
    "        # Add each artistâ€™s name and associated link to a row\n",
    "        f.writerow([title, authors, text, image, video, link, publish_date])\n",
    "except Exception as e: print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
